# backend/app/api/v1/papers.py
from __future__ import annotations
from pathlib import Path
from typing import Optional, List, Dict, Any

from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from pydantic import BaseModel, Field
from loguru import logger

from sqlalchemy import delete, or_, and_
from sqlmodel import select

from ..deps import SessionDep
from ...models import (
    Paper, Tag, Author,
    PaperTagLink, PaperAuthorLink, Note,
    PaperFolderLink,   # éœ€è¦æœ‰è¯¥æ¨¡å‹ï¼ˆç”¨äºç›®å½•è¿‡æ»¤/åˆ†é…ï¼‰
)
from ...schemas import PaperRead, PaperUpdate
from ...core.config import settings
from ...services.pdf_parser import parse_pdf_metadata
from ...services.doi_resolver import fetch_by_doi, DoiResolveError

router = APIRouter()

def _norm_title(s: Optional[str]) -> str:
    if not s:
        return ""
    import re
    s = s.lower()
    s = re.sub(r"[^a-z0-9]+", "", s)
    return s

# venue ç¼©å†™æ˜ å°„ï¼ˆä¸å‰ç«¯ä¸€è‡´ï¼‰
import re as _re
_VENUE_ABBR_PATTERNS: list[tuple[_re.Pattern[str], str]] = [
    (_re.compile(r"(international symposium on microarchitecture|(^|\W)micro(\W|$))", _re.I), "MICRO"),
    (_re.compile(r"programming language design and implementation|(^|\W)pldi(\W|$)", _re.I), "PLDI"),
    (_re.compile(r"international symposium on computer architecture|(^|\W)isca(\W|$)", _re.I), "ISCA"),
    (_re.compile(r"architectural support for programming languages|(^|\W)asplos?(\W|$)", _re.I), "ASPLOS"),
    (_re.compile(r"transactions on architecture and code optimization|(^|\W)taco(\W|$)", _re.I), "TACO"),
    (_re.compile(r"transactions on design automation of electronic systems|(^|\W)todaes(\W|$)", _re.I), "TODAES"),
    (_re.compile(r"design automation conference|(^|\W)dac(\W|$)", _re.I), "DAC"),
    (_re.compile(r"neurips|nips", _re.I), "NeurIPS"),
    (_re.compile(r"international conference on machine learning|(^|\W)icml(\W|$)", _re.I), "ICML"),
    (_re.compile(r"computer vision and pattern recognition|(^|\W)cvpr(\W|$)", _re.I), "CVPR"),
    (_re.compile(r"international conference on computer vision|(^|\W)iccv(\W|$)", _re.I), "ICCV"),
    (_re.compile(r"european conference on computer vision|(^|\W)eccv(\W|$)", _re.I), "ECCV"),
    (_re.compile(r"very large data bases|(^|\W)vldb(\W|$)", _re.I), "VLDB"),
    (_re.compile(r"sigmod", _re.I), "SIGMOD"),
    (_re.compile(r"the web conference|(^|\W)www(\W|$)", _re.I), "WWW"),
    (_re.compile(r"supercomputing|(^|\W)sc(\W|$)", _re.I), "SC"),
    (_re.compile(r"siggraph", _re.I), "SIGGRAPH"),
]
def _venue_abbr(name: str | None) -> str | None:
    if not name: return None
    for pat, abbr in _VENUE_ABBR_PATTERNS:
        if pat.search(name): return abbr
    return None    

# è¯†åˆ« arXiv IDï¼ˆä»…ç”¨äºå¼±æç¤º/å…œåº•ï¼Œä¸å‘èµ·ç½‘ç»œè¯·æ±‚ï¼‰
def _guess_arxiv_id_from_filename(name: str) -> str | None:
    m = _re.search(r"(\d{4}\.\d{4,5})(v\d+)?", name)
    if m: return m.group(1)
    m = _re.search(r"([a-z\-]+/\d{7})", name, _re.I)  # legacy: cs/9901001
    if m: return m.group(1)
    return None

def _paper_payload(session: SessionDep, paper_id: int) -> Dict[str, Any]:
    paper = session.get(Paper, paper_id)
    if not paper:
        raise HTTPException(status_code=404, detail="Not Found")

    # tag ids
    tag_ids = [ln.tag_id for ln in session.exec(select(PaperTagLink).where(PaperTagLink.paper_id == paper_id))]
    # authorsï¼ˆå±•å¼€ï¼‰
    author_ids = [ln.author_id for ln in session.exec(select(PaperAuthorLink).where(PaperAuthorLink.paper_id == paper_id))]
    authors_payload: List[Dict[str, Any]] = []
    if author_ids:
        authors = list(session.exec(select(Author).where(Author.id.in_(author_ids))))
        for a in authors:
            authors_payload.append({
                "id": getattr(a, "id", None),
                "name": getattr(a, "name", None),
                "orcid": getattr(a, "orcid", None) if hasattr(a, "orcid") else None,
                "affiliation": getattr(a, "affiliation", None) if hasattr(a, "affiliation") else None,
            })

    # ä»…ç”¨äºæ—¥å¿—å‹å¥½
    tag_names = [t.name for t in session.exec(
        select(Tag).join(PaperTagLink, PaperTagLink.tag_id == Tag.id).where(PaperTagLink.paper_id == paper_id)
    )]
    logger.info(f"[payload] paper#{paper_id} -> tag_ids={tag_ids}, tag_names={tag_names}")

    return {
        "id": paper.id,
        "title": paper.title,
        "abstract": paper.abstract,
        "year": paper.year,
        "doi": paper.doi,
        "venue": paper.venue,
        "pdf_url": paper.pdf_url,
        "tag_ids": tag_ids,
        "author_ids": author_ids,
        "authors": authors_payload,
    }

@router.get("/", response_model=list[PaperRead])
def list_papers(
    session: SessionDep,
    q: Optional[str] = None,
    tag_id: Optional[int] = None,
    folder_id: Optional[int] = None,
    year_min: Optional[int] = None,   # âœ… æ–°å¢
    year_max: Optional[int] = None,   # âœ… æ–°å¢
    venue: Optional[str] = None,      # âœ… æ–°å¢
    venue_abbr: Optional[str] = None,   # ğŸ‘ˆ æ–°å¢
    dedup: bool = True,
):
    stmt = select(Paper)
    if q:
        qlike = f"%{q}%"
        stmt = stmt.where(or_(Paper.title.ilike(qlike), Paper.venue.ilike(qlike), Paper.doi.ilike(qlike)))
    if tag_id is not None:
        stmt = stmt.join(PaperTagLink, PaperTagLink.paper_id == Paper.id).where(PaperTagLink.tag_id == tag_id)
    if folder_id is not None:
        stmt = stmt.join(PaperFolderLink, PaperFolderLink.paper_id == Paper.id).where(PaperFolderLink.folder_id == folder_id)

    # å…ˆæ”¶é›†è¦åŠ çš„ where æ¡ä»¶
    conds = []

    if venue:
        conds.append(Paper.venue.ilike(f"%{venue}%"))

    # åªæœ‰å½“è‡³å°‘ä¸€ä¸ªè¾¹ç•Œè¢«æä¾›æ—¶æ‰ç­›å¹´ä»½
    if (year_min is not None) or (year_max is not None):
        rng = []
        if year_min is not None:
            rng.append(Paper.year >= year_min)
        if year_max is not None:
            rng.append(Paper.year <= year_max)

        # year ä¸º NULL æ°¸è¿œæ”¾è¡Œï¼›å¦åˆ™å¿…é¡»è½åœ¨ç»™å®šèŒƒå›´ï¼ˆå“ªæ€•åªç»™äº†ä¸€ä¸ªè¾¹ç•Œï¼‰
        if rng:
            conds.append(or_(Paper.year.is_(None), and_(*rng)))

    # ç»Ÿä¸€è½åˆ° stmt
    for c in conds:
        stmt = stmt.where(c)

    stmt = stmt.order_by(Paper.created_at.desc())

    rows = list(session.exec(stmt))
    if venue_abbr:
        wanted = {x.strip().upper() for x in venue_abbr.split(",") if x.strip()}
        rows = [p for p in rows if (_venue_abbr(getattr(p, "venue", None)) or "").upper() in wanted]
    result = []
    seen = set()
    for p in rows:
        key = p.doi or _norm_title(p.title)
        if dedup and key in seen:
            continue
        seen.add(key)
        result.append(_paper_payload(session, p.id))
    return result

@router.get("/{paper_id}", response_model=PaperRead)
def get_paper(paper_id: int, session: SessionDep):
    return _paper_payload(session, paper_id)

@router.patch("/{paper_id}", response_model=PaperRead)
def update_paper(paper_id: int, data: PaperUpdate, session: SessionDep):
    paper = session.get(Paper, paper_id)
    if not paper:
        raise HTTPException(status_code=404, detail="Not Found")
    for field, value in data.model_dump(exclude_unset=True).items():
        if field in {"tag_ids", "author_ids"}:
            continue
        setattr(paper, field, value)
    session.add(paper)
    session.commit()
    session.refresh(paper)
    return _paper_payload(session, paper_id)

def _safe_write_upload(dest: Path, content: bytes) -> Path:
    i = 0
    final = dest
    while final.exists():
        i += 1
        final = dest.with_name(f"{dest.stem}_{i}{dest.suffix}")
    final.write_bytes(content)
    return final

def _merge_paper_fields(paper: Paper, data: Dict[str, Any]) -> None:
    for key in ["title", "abstract", "year", "doi", "venue", "pdf_url"]:
        val = data.get(key)
        if val is None or val == "":
            continue
        current = getattr(paper, key, None)
        if current in (None, "", 0):
            logger.info(f"[_merge_paper_fields] setting {key} from {current!r} -> {val!r}")
            setattr(paper, key, val)
        else:
            logger.info(f"[_merge_paper_fields] keep existing {key}={current!r}, skip new {val!r}")

@router.post("/upload", response_model=PaperRead)
async def upload_paper(
    session: SessionDep,
    file: UploadFile = File(...),
    title: Optional[str] = Form(None),
    abstract: Optional[str] = Form(None),
    year: Optional[int] = Form(None),
    doi: Optional[str] = Form(None),
    venue: Optional[str] = Form(None),
    tag_ids: Optional[list[int]] = Form(None),
    author_ids: Optional[list[int]] = Form(None),
):
    return await _upload_one(
        session=session, file=file,
        title=title, abstract=abstract, year=year, doi=doi, venue=venue,
        tag_ids=tag_ids, author_ids=author_ids,
    )


@router.post("/upload/batch", response_model=list[PaperRead])
async def upload_batch(session: SessionDep, files: list[UploadFile] = File(...)):
    out: list[PaperRead] = []
    for f in files:
        tmp = await _upload_one(
            session=session, file=f,
            title=None, abstract=None, year=None, doi=None, venue=None,
            tag_ids=None, author_ids=None,
        )
        out.append(tmp)
    return out

from pydantic import BaseModel, Field

class PaperCreateSimple(BaseModel):
    title: Optional[str] = None
    abstract: Optional[str] = None
    year: Optional[int] = None
    doi: Optional[str] = None
    venue: Optional[str] = None
    tag_ids: Optional[list[int]] = None
    author_ids: Optional[list[int]] = None

@router.post("/create", response_model=PaperRead)
async def create_paper_simple(session: SessionDep, payload: PaperCreateSimple):
    data = payload.model_dump(exclude_unset=True)
    norm_doi = (data.get("doi") or "").strip()

    # ä¼ äº† DOIï¼šå¿…é¡»å…ˆè§£ææˆåŠŸæ‰å…è®¸å…¥åº“
    resolved: Dict[str, Any] = {}
    if norm_doi:
        try:
            resolved = await fetch_by_doi(norm_doi)
        except Exception as e:
            # è§£æå¤±è´¥ï¼šä¸å…¥åº“ï¼Œæ˜¾å¼å‘Šè¯‰å‰ç«¯
            raise HTTPException(status_code=424, detail=f"DOI è§£æå¤±è´¥ï¼š{e}")  # 424 Failed Dependency

        # åŸºæœ¬å®Œæ•´æ€§æ ¡éªŒï¼šè‡³å°‘è¦æœ‰æ ‡é¢˜
        if not resolved.get("title"):
            raise HTTPException(status_code=424, detail="DOI è§£æä¸å®Œæ•´ï¼ˆç¼ºå°‘æ ‡é¢˜ï¼‰ï¼Œå·²å–æ¶ˆå…¥åº“")

    # åˆå¹¶â€œå‰ç«¯æ˜¾å¼ > è§£æå€¼â€
    title_final    = (data.get("title") or resolved.get("title"))
    abstract_final = (data.get("abstract") or None)   # Crossref æŠ½è±¡å¾ˆå°‘ï¼Œå…è®¸ä¸ºç©º
    year_final     = (data.get("year") or resolved.get("year"))
    venue_final    = (data.get("venue") or resolved.get("venue"))

    # æ²¡ DOI çš„çº¯æ‰‹å¡«ï¼šè‡³å°‘å¾—æœ‰ title
    if not norm_doi and not title_final:
        raise HTTPException(status_code=422, detail="ç¼ºå°‘æ ‡é¢˜ï¼›æ—  DOI çš„æƒ…å†µä¸‹å¿…é¡»æä¾›æ ‡é¢˜")

    # å»é‡ï¼šåŒ DOI åˆå¹¶ï¼Œå¦åˆ™æ–°å»º
    paper = session.exec(select(Paper).where(Paper.doi == norm_doi)).first() if norm_doi else None
    if paper:
        _merge_paper_fields(paper, {
            "title": title_final,
            "abstract": abstract_final,
            "year": year_final,
            "doi": norm_doi or None,
            "venue": venue_final,
        })
        session.add(paper); session.commit(); session.refresh(paper)
    else:
        paper = Paper(
            title=title_final or "Untitled",   # ç†è®ºä¸Šä¸ä¼šèµ°åˆ°è¿™é‡Œï¼ˆä¸Šé¢å·²æ ¡éªŒï¼‰
            abstract=abstract_final,
            year=year_final,
            doi=norm_doi or None,
            venue=venue_final,
            pdf_url=None,
        )
        session.add(paper); session.commit(); session.refresh(paper)

    # è§£æåˆ°çš„ä½œè€… + æ˜¾å¼ä¼ å…¥çš„ author_ids
    created_ids: list[int] = []
    for am in (resolved.get("authors") or []):
        name = (am or {}).get("name")
        if not name: continue
        aff = (am or {}).get("affiliation")
        a = session.exec(select(Author).where(Author.name == name)).first()
        if not a:
            try: a = Author(name=name, affiliation=aff)
            except TypeError: a = Author(name=name)
            session.add(a); session.commit(); session.refresh(a)
        else:
            if aff and hasattr(a, "affiliation") and not getattr(a, "affiliation"):
                a.affiliation = aff; session.add(a); session.commit()
        created_ids.append(a.id)

    explicit_ids = set(payload.author_ids or [])
    final_author_ids = list({*explicit_ids, *created_ids})
    if final_author_ids:
        session.exec(delete(PaperAuthorLink).where(PaperAuthorLink.paper_id == paper.id))
        for aid in final_author_ids:
            session.add(PaperAuthorLink(paper_id=paper.id, author_id=aid))

    if payload.tag_ids:
        session.exec(delete(PaperTagLink).where(PaperTagLink.paper_id == paper.id))
        for tid in payload.tag_ids:
            session.add(PaperTagLink(paper_id=paper.id, tag_id=tid))

    session.commit()
    return _paper_payload(session, paper.id)

    
class NotePayload(BaseModel):
    content: str = Field("", description="çº¯æ–‡æœ¬å†…å®¹")

@router.get("/{paper_id}/note")
def get_note(paper_id: int, session: SessionDep):
    n = session.exec(select(Note).where(Note.paper_id == paper_id)).first()
    val = ""
    if n is not None:
        if hasattr(n, "content"): val = getattr(n, "content") or ""
        elif hasattr(n, "text"):  val = getattr(n, "text") or ""
    return {"paper_id": paper_id, "content": val}

@router.put("/{paper_id}/note")
def put_note(paper_id: int, payload: NotePayload, session: SessionDep):
    n = session.exec(select(Note).where(Note.paper_id == paper_id)).first()
    if n is None:
        # å…¼å®¹ä¸åŒå­—æ®µåï¼ˆcontent/textï¼‰
        try:        n = Note(paper_id=paper_id, content=payload.content)
        except: 
            try:    n = Note(paper_id=paper_id, text=payload.content)
            except:
                    n = Note(paper_id=paper_id); 
                    if hasattr(n, "content"): setattr(n, "content", payload.content)
                    elif hasattr(n, "text"):  setattr(n, "text", payload.content)
        session.add(n)
    else:
        if hasattr(n, "content"): n.content = payload.content
        elif hasattr(n, "text"):  n.text = payload.content
        session.add(n)
    session.commit()
    return {"ok": True}

class TagNames(BaseModel):
    tags: list[str]

@router.put("/{paper_id}/tags", response_model=PaperRead)
def update_tags_by_names(paper_id: int, payload: TagNames, session: SessionDep):
    logger.info(f"[tags.put] paper={paper_id} incoming={payload.model_dump()}")
    paper = session.get(Paper, paper_id)
    if not paper:
        raise HTTPException(status_code=404, detail="Not Found")

    names = [t.strip() for t in (payload.tags or []) if t and t.strip()]
    # å»é‡ï¼Œä¿æŒæ¬¡åº
    seen = set(); norm: list[str] = []
    for n in names:
        if n not in seen:
            seen.add(n); norm.append(n)
    logger.info(f"[tags.put] normalized={norm}")

    # æ‰¾å·²æœ‰
    existing = list(session.exec(select(Tag).where(Tag.name.in_(norm)))) if norm else []
    name2tag = {t.name: t for t in existing}

    # æ–°å»ºç¼ºå¤±
    created_ids: list[int] = []
    for n in norm:
        if n not in name2tag:
            t = Tag(name=n)
            session.add(t); session.commit(); session.refresh(t)
            name2tag[n] = t
            created_ids.append(t.id)

    # é‡æ–°å…³è”
    logger.info(f"[tags.put] unlink old links for paper={paper_id}")
    session.exec(delete(PaperTagLink).where(PaperTagLink.paper_id == paper_id))
    linked_ids: list[int] = []
    for n in norm:
        tid = name2tag[n].id
        session.add(PaperTagLink(paper_id=paper_id, tag_id=tid))
        linked_ids.append(tid)

    session.commit()
    logger.info(f"[tags.put] created={created_ids} linked_ids={linked_ids}")

    # è¿”å›æœ€æ–° payload
    payload_after = _paper_payload(session, paper_id)
    logger.info(f"[tags.put] after-commit tag_ids={payload_after['tag_ids']}")
    return payload_after

@router.delete("/{paper_id}")
def delete_paper(paper_id: int, session: SessionDep):
    paper = session.get(Paper, paper_id)
    if not paper:
        raise HTTPException(status_code=404, detail="Not Found")
    session.exec(delete(PaperTagLink).where(PaperTagLink.paper_id == paper_id))
    session.exec(delete(PaperAuthorLink).where(PaperAuthorLink.paper_id == paper_id))
    session.exec(delete(Note).where(Note.paper_id == paper_id))
    # è§£é™¤ç›®å½•å…³ç³»
    session.exec(delete(PaperFolderLink).where(PaperFolderLink.paper_id == paper_id))
    try:
        if paper.pdf_url and paper.pdf_url.startswith("/files/"):
            rel = paper.pdf_url.replace("/files/", "")
            target = Path(settings.STORAGE_DIR) / rel
            if target.exists():
                target.unlink()
    except Exception:
        pass
    session.delete(paper); session.commit()
    logger.info(f"[delete] paper#{paper_id} removed")
    return {"ok": True}

# æ”¾åœ¨ _merge_paper_fields åé¢
async def _upload_one(
    session: SessionDep,
    file: UploadFile,
    title: Optional[str] = None,
    abstract: Optional[str] = None,
    year: Optional[int] = None,
    doi: Optional[str] = None,
    venue: Optional[str] = None,
    tag_ids: Optional[list[int]] = None,
    author_ids: Optional[list[int]] = None,
) -> PaperRead:
    pdf_dir = Path(settings.STORAGE_DIR) / "pdfs"
    pdf_dir.mkdir(parents=True, exist_ok=True)
    raw = await file.read()
    dest = _safe_write_upload(pdf_dir / (file.filename or "upload.pdf"), raw)
    pdf_url = f"/files/pdfs/{dest.name}"

    meta: Dict[str, Any] = {}
    try:
        meta = await parse_pdf_metadata(str(dest))
    except Exception as e:
        logger.warning(f"pdf parse failed: {e}")
        meta = {}

    data = {
        "title": title or meta.get("title") or (file.filename or dest.name),
        "abstract": abstract or meta.get("abstract"),
        "year": year or meta.get("year"),
        "doi": (doi or meta.get("doi")),
        "venue": venue or meta.get("venue"),
        "pdf_url": pdf_url,
    }

    # PDF æå–çš„ DOI å®¹æ˜“è¯¯æŠ“åˆ°å‚è€ƒæ–‡çŒ®é‡Œçš„ DOIï¼›ä»…åœ¨ä¸æ ‡é¢˜åŒ¹é…æ—¶æ‰ä¿¡ä»»ï¼Œå¹¶ç”¨ DOI åæŸ¥è¡¥å…¨å­—æ®µ
    norm_doi = (data.get("doi") or "").strip()
    if norm_doi:
        try:
            resolved = await fetch_by_doi(norm_doi)
            rt = (resolved.get("title") or "")
            mt = (data.get("title") or "")
            nt_r = _norm_title(rt)
            nt_m = _norm_title(mt)
            match_ok = bool(nt_r and nt_m and (nt_r in nt_m or nt_m in nt_r))
            if not match_ok:
                logger.warning(f"[upload] dropping DOI from PDF ({norm_doi}) â€“ title mismatch: parsed='{mt}' vs resolved='{rt}'")
                norm_doi = ""
            else:
                # åæŸ¥æˆåŠŸï¼šç¼ºå•¥è¡¥å•¥ï¼ˆå‰ç«¯æ˜¾å¼/è§£æä¼˜å…ˆçº§å·²åœ¨ create API ç»Ÿä¸€ï¼‰
                if not data.get("title") and resolved.get("title"):   data["title"] = resolved["title"]
                if not data.get("year") and resolved.get("year") is not None: data["year"] = resolved["year"]
                if not data.get("venue") and resolved.get("venue"):   data["venue"] = resolved["venue"]
        except Exception as e:
            logger.warning(f"[upload] DOI resolve failed for '{norm_doi}': {e}")

    # å¦‚æœç–‘ä¼¼ arXiv ä¸”æ²¡æœ‰å¯é  DOIï¼Œåˆ™æŠŠ venue æ ‡æˆ arXivï¼ˆéç ´åæ€§ï¼Œç”¨äºå‰ç«¯ç­›é€‰ä¸è§†è§‰æç¤ºï¼‰
    if not norm_doi:
        if (data.get("venue") and "arxiv" in (data.get("venue") or "").lower()) or _guess_arxiv_id_from_filename(dest.name):
            data["venue"] = data.get("venue") or "arXiv"

    data["doi"] = norm_doi or None

    existing = None
    if data.get("doi"):
        existing = session.exec(select(Paper).where(Paper.doi == data["doi"])).first()
    if existing:
        _merge_paper_fields(existing, data)
        session.add(existing); session.commit()
        paper = existing
    else:
        paper = Paper(**data)
        session.add(paper); session.commit(); session.refresh(paper)

    # è§£æä½œè€…
    authors_meta = meta.get("authors") or []
    explicit_ids = set(author_ids or [])
    created_ids: List[int] = []
    for am in authors_meta:
        name = (am or {}).get("name")
        if not name: continue
        aff = (am or {}).get("affiliation")
        a = session.exec(select(Author).where(Author.name == name)).first()
        if not a:
            try:
                a = Author(name=name, affiliation=aff)
            except TypeError:
                a = Author(name=name)
            session.add(a); session.commit(); session.refresh(a)
        else:
            if aff and hasattr(a, "affiliation") and not getattr(a, "affiliation"):
                a.affiliation = aff; session.add(a); session.commit()
        created_ids.append(a.id)

    final_author_ids = list({*explicit_ids, *created_ids})
    if final_author_ids:
        session.exec(delete(PaperAuthorLink).where(PaperAuthorLink.paper_id == paper.id))
        for aid in final_author_ids:
            session.add(PaperAuthorLink(paper_id=paper.id, author_id=aid))

    if tag_ids:
        session.exec(delete(PaperTagLink).where(PaperTagLink.paper_id == paper.id))
        for tid in tag_ids:
            session.add(PaperTagLink(paper_id=paper.id, tag_id=tid))

    session.commit()
    logger.info(f"[upload] paper#{paper.id} saved file={dest.name} doi={paper.doi}")
    logger.info(f"[upload_one] final paper id={paper.id} doi={paper.doi!r} pdf_url={getattr(paper, 'pdf_url', None)!r}")
    return _paper_payload(session, paper.id)